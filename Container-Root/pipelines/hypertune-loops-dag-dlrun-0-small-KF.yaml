# Example of loops using DAGs
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: hypertune-loops-dag-
spec:
  entrypoint: hypertune-loops-dag

  arguments:
    parameters:
    - name: arch
      value: nvidia-tesla-v100

  volumes:
  - name: workdir
    persistentVolumeClaim:
      claimName: pvc-dsa-kf-hypertune

  templates:
  - name: hypertune-loops-dag
    dag:
      tasks:
      - name: bayesopt0
        #dependencies: [dlrun0]
        template: hypertune
        arguments:
          parameters:
          - {name: num, value: "Matern52_iter0"}
      - name: dlrun1
        dependencies: [bayesopt0]
        template: dlmodel
        arguments:
          parameters:
          - {name: num, value: "{{item}}"}
        withItems: ["24_0", "24_1", "24_2", "24_3", "24_4"]

  - name: hypertune
    activeDeadlineSeconds: 100000
    daemon: true
    inputs:
      parameters:
      - name: num
    container:
      image: registry-dev.bhge.ai/dsa/hypertune/bayesgp:CURRENT-SNAPSHOT
      imagePullPolicy: Always
      command: [python3, -c]
      args: ["import numpy as np; import os; import tensorflow;  cur_str='--- hello world from trial in hypertune'; num_str=str(np.random.randint(low=0,high=10000)); print(cur_str + num_str); os.system('cd /hostfiles/Data; pwd; ls -ltr')"]
      #command: [bash, -c]
      #args: ["cd /hostfiles/Data/; source utility_functions.sh; run_bayesopt_all_steps \"{{inputs.parameters.num}}\""]
      volumeMounts:
      - name: workdir
        mountPath: /hostfiles/
      resources:
        limits:
          nvidia.com/gpu: 1 # from Allocatable: using kubectl describe node <machine instance with gpu>

  - name: dlmodel
    daemon: true
    inputs:
      parameters:
      - name: num
    activeDeadlineSeconds: 100000
    container:
      #ADD arguments for tensorflow docker ro run nvidia
      image: registry-dev.bhge.ai/dsa/hypertune/tensorflow:nightly-gpu-py3-mod
      imagePullPolicy: Always
      command: [bash, -c]
      #args: ["cd /hostfiles/Data/; source utility_functions.sh; submit_job \"{{inputs.parameters.num}}\""]
      args: ["cd /hostfiles/Data/; CUR_CASE=\"{{inputs.parameters.num}}\";    CUR_CASE_NUM=`echo $CUR_CASE | awk -F'_' '{print $1}'`; CUR_FOLD_NUM=`echo $CUR_CASE | awk -F'_' '{print $2}'` ;  python ./autoencoder_train.py ${CUR_CASE_NUM} ${CUR_FOLD_NUM} 1> ./nohup_n${CUR_CASE_NUM}_f${CUR_FOLD_NUM}.out  2> ./nohup_n${CUR_CASE_NUM}_f${CUR_FOLD_NUM}.err  "]
      volumeMounts:
      - name: workdir
        mountPath: /hostfiles/
      resources:
        limits:
          nvidia.com/gpu: 1 # from Allocatable: using kubectl describe node <machine instance with gpu>
